{
  "hash": "58cc3699d7315db7896e319f449ec41c",
  "result": {
    "markdown": "---\ntitle: \"Detecting OCR mistakes in text using BERT for token classification\"\ndate: \"2022-10-21\"\ncategories: [\"ocr post-correction\"]\nimage: \"woman_with_newspaper.png\"\n---\n\nSome years ago, I did a\n[project with the Dutch National Library on OCR post-correction](https://lab.kb.nl/about-us/blog/newspaper-ocr-quality-what-have-we-learned).\nI wanted to investigate the potential of Deep Learning for correcting OCR errors\nin text. For various reasons,\n[we never got very good results](https://docs.google.com/document/d/1ui1wFNwIcnTn5gLvDL8JnM7epsod1uVAZNo31Zg_YuM/edit).\nAround the same time, two\n[competitions on post-OCR text correction](https://sites.google.com/view/icdar2019-postcorrectionocr)\nwere organized at the ICDAR conference\n([2017](https://sites.google.com/view/icdar2017-postcorrectionocr) and\n[2019](https://sites.google.com/view/icdar2019-postcorrectionocr)).\nI remained interested in the problem and started working on reproducing the\ncompetition results in my free time.\n\n[![Interior with a woman reading the newspaper, by Gerke Henkes (1854 - 1927)](woman_with_newspaper.png)](http://hdl.handle.net/10934/RM0001.COLLECT.247881)\n\nThe competition divided the challenge of OCR post-correction into two tasks:\n\n1. Detection\n2. Correction\n\nThis post is about my first experiences with solving the detection task.\nThe [paper about the results](https://ieeexplore.ieee.org/abstract/document/8978127)\ncontains very brief descriptions of the competitors' solutions, which makes it\nhard to reproduce their models. The paper states that the winner used\nmultilingual BERT with CNN layers for recognizing tokens with OCR mistakes.\nFor simplicity, I decided to start with training a simpler BERT for token\nclassification model.\n\n## The data\n\nThe [competition dataset](https://sites.google.com/view/icdar2019-postcorrectionocr/dataset)\nconsists of (historical) newspaper data in 10 languages. Each text file contains\nthree lines, e.g.,\n\n```\n[OCR_toInput] This is a cxample...\n[OCR_aligned] This is a@ cxample...\n[ GS_aligned] This is an example.@@\n```\n\nThe first line contains the ocr input text. The second line contains the aligned\nocr and the third line contains the aligned gold standard (GS). `@` is the\naligment character and `#` represents tokens in the OCR that do not occur in the\ngold standard (noise).\n\nTask 1 of the competition is about finding tokens with OCR mistakes. In this\ncontext, a token refers to a string between two whitespaces. The goal of this\ntask is to predict the position and length of OCR mistakes. I created a Python\nlibrary called [ocrpostcorrection](https://github.com/jvdzwaan/ocrpostcorrection)\nthat contains functionality for doing OCR postcorrection, including converting\nthe ICDAR dataset into a Hugging Face dataset with 'sentences' of a certain\nlength. This\n[notebook](https://github.com/jvdzwaan/ocrpostcorrection-notebooks/blob/main/local/icdar-create-hf-dataset.ipynb)\ncontains the code used to create the dataset. I will now explain the most\nimportant steps.\n\nFirst, a text is divided into aligned tokens by splitting the aligned OCR and GS\non matching whitespaces. The ocrpostcorrection library contains a\n[dataclass](https://docs.python.org/3/library/dataclasses.html)\n[`AlignedToken`](https://jvdzwaan.github.io/ocrpostcorrection/icdar_data.html#alignedtoken)\nwhich is used to store the results:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom dataclasses import dataclass\n\n@dataclass\nclass AlignedToken:\n    ocr: str  # String in the OCR text (excluding aligmnent characters)\n    gs: str  # String in the gold standard (excluding aligmnent characters)\n    ocr_aligned: str  # String in the aligned OCR text (including aligmnent characters)\n    gs_aligned: str  # String in the aligned GS text (including aligmnent characters)\n    start: int  # The index of the first character in the OCR text\n    len_ocr: int  # The lentgh of the OCR string\n```\n:::\n\n\nThe [tokenize_aligned](tokenize_aligned) function is used to divide an input\ntext into `AlignedToken`s.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom ocrpostcorrection.icdar_data import tokenize_aligned\n\ntokenize_aligned('This is a@ cxample...', 'This is an example.@@')\n```\n\n::: {.cell-output .cell-output-display execution_count=72}\n```\n[AlignedToken(ocr='This', gs='This', ocr_aligned='This', gs_aligned='This', start=0, len_ocr=4),\n AlignedToken(ocr='is', gs='is', ocr_aligned='is', gs_aligned='is', start=5, len_ocr=2),\n AlignedToken(ocr='a', gs='an', ocr_aligned='a@', gs_aligned='an', start=8, len_ocr=1),\n AlignedToken(ocr='cxample...', gs='example.', ocr_aligned='cxample...', gs_aligned='example.@@', start=10, len_ocr=10)]\n```\n:::\n:::\n\n\nThe OCR text of an `AlignedToken` may still consist of multiple tokens. This is\nthe case when the OCR text contains one or more spaces. To make sure the\n(sub)tokenization of a token is the same, no matter if it was not yet tokenized\ncompletely, another round of tokenization is added. Using the\n[get_input_tokens](https://jvdzwaan.github.io/ocrpostcorrection/icdar_data.html#get_input_tokens)\nfunction, every `AlignedToken` is split on whitespace. Each subtoken is stored\nin the `InputToken` dataclass:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom dataclasses import dataclass\n\n@dataclass\nclass InputToken:\n    ocr: str  # OCR text\n    gs: str  # GS text\n    start: int  # character offset in the original OCR text\n    len_ocr: int  # length of the OCR text\n    label: int  # Class label: [0, 1, 2]\n```\n:::\n\n\nThis dataclass also adds the class labels. There are three classes:\n\n* 0: No OCR mistake\n* 1: Start token of an OCR mistake\n* 2: Inside token of an OCR mistake\n\nThis example code shows how an `AlignedToken` is divided into `inputToken`s:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfrom ocrpostcorrection.icdar_data import AlignedToken, get_input_tokens\n\nt = AlignedToken('Long ow.', 'Longhow.', 'Long ow.', 'Longhow.', 24, 8)\nprint(t)\n\nfor inp_tok in get_input_tokens(t):\n    print(inp_tok)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAlignedToken(ocr='Long ow.', gs='Longhow.', ocr_aligned='Long ow.', gs_aligned='Longhow.', start=24, len_ocr=8)\nInputToken(ocr='Long', gs='Longhow.', start=24, len_ocr=4, label=1)\nInputToken(ocr='ow.', gs='', start=29, len_ocr=3, label=2)\n```\n:::\n:::\n\n\nThe output produced by this code is:\n\n```\nAlignedToken(ocr='Long ow.', gs='Longhow.', ocr_aligned='Long ow.', gs_aligned='Longhow.', start=24, len_ocr=8)\nInputToken(ocr='Long', gs='Longhow.', start=24, len_ocr=4, label=1)\nInputToken(ocr='ow.', gs='', start=29, len_ocr=3, label=2)\n```\n\nA text can be tokenized by combining the `tokenize_aligned` and\n`get_input_tokens` functions. Texts are stored in another dataclass:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfrom dataclasses import dataclass\n\n@dataclass\nclass Text:\n    ocr_text: str  # OCR input text\n    tokens: list  # List of AlignedTokens\n    input_tokens: list  # List of InputTokens\n    score: float  # Normalized editdistance between OCR and GS text\n```\n:::\n\n\nA text file can be tokenized using the function\n[process_text](https://jvdzwaan.github.io/ocrpostcorrection/icdar_data.html#process_text):\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom pathlib import Path\nfrom ocrpostcorrection.icdar_data import process_text\n\nin_file = Path('example.txt')\ntext = process_text(in_file)\n```\n:::\n\n\nwhich results in the following instance of the `Text` dataclass:\n\n```\nText(ocr_text='This is a cxample...',\n     tokens=[AlignedToken(ocr='This', gs='This', ocr_aligned='This', gs_aligned='This', start=0, len_ocr=4),\n             AlignedToken(ocr='is', gs='is', ocr_aligned='is', gs_aligned='is', start=5, len_ocr=2),\n             AlignedToken(ocr='a', gs='an', ocr_aligned='a@', gs_aligned='an', start=8, len_ocr=1),\n             AlignedToken(ocr='cxample...', gs='example.', ocr_aligned='cxample...', gs_aligned='example.@@', start=10, len_ocr=10)],\n     input_tokens=[InputToken(ocr='This', gs='This', start=0, len_ocr=4, label=0),\n                   InputToken(ocr='is', gs='is', start=5, len_ocr=2, label=0),\n                   InputToken(ocr='a', gs='an', start=8, len_ocr=1, label=1),\n                   InputToken(ocr='cxample...', gs='example.', start=10, len_ocr=10, label=1)],\n     score=0.2)\n```\n\nThe next step is processing the entire dataset. This can be done with the\n[generate_data](https://jvdzwaan.github.io/ocrpostcorrection/icdar_data.html#generate_data)\nfunction. The ouptut of this function consists of a dictionary containing\n`<file name>: Text` pairs and a pandas DataFrame containing metadata. For each\nfile, the metadata contains file name, language, score (normalized\neditdistance), and the numbers of aligned and input tokens:\n\n|\t| language |\tfile_name |\tscore |\tnum_tokens |\tnum_input_tokens |\n| - | - | - | - | - | - |\n| 0\t| SL\t| SL/SL1/29.txt | \t0.463415 |\t7 |\t7 |\n| 1\t| SL\t| SL/SL1/15.txt |\t0.773294 |\t155 |\t246 |\n| 2\t| SL\t| SL/SL1/114.txt |\t0.019256 |\t268 |\t272 |\n\nThe train set consists of 11662 text files. The mean number of `InputToken`s is\n269.51, with a standard deviation of 200.61. The minimum number of `InputToken`s\nis 0 and the maximum 3068. The histogram below shows the distribution of the\nnumber of `InputToken`s. Most texts have less than 250 `InputToken`s and there\nare some very long texts.\n\n[![](numbers_of_input_tokens_in_icdar_train_set.png)]()\n\nThe mean normalized editdistance between OCR and GS text is 0.21, with a\nstandard deviation of 0.13. The minimum is 0.00 and the maximum is 1.00. Smaller\ndistances are better (less OCR mistakes). The distribution of normalized\neditdistance shows two peaks; one close to zero and one between 0.2 and 0.3.\nMost texts have a low editdistance. This means that most texts should be of high\nenough quality to be able to learn from.\n\n[![](normalized_editdistance_in_train_set.png)]()\n\nThe ICDAR dataset consists of a train and test set. For validation, I split\noff 10% of the texts from the train set, stratified on language.\n\nBecause BERT has a limit on input length and the length of the texts vary, the\ntexts are split up in smaller sequences. As an approximation of sentence length,\nfor this first experiment, I chose a sequence length of 35 tokens (with an\noverlap of 5 tokens). The\n[generate_sentence](https://jvdzwaan.github.io/ocrpostcorrection/icdar_data.html#generate_sentences)\nfunction returns sequences of a certain length and overlap, given the metadata\nDataFrame and dictionary of `Text` instances.\n\nThe sequences are returned as pandas DataFrame, which can be converted to a\n[Hugging Face Dataset](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset)\nusing the\n[Dataset.from_pandas()](https://huggingface.co/docs/datasets/v2.6.0/en/package_reference/main_classes#datasets.Dataset.from_pandas)\nmethod. The first two 'sentences' in the train set look like:\n\n```\n{\n    'key': 'FR/FR1/499.txt',\n    'start_token_id': 0,\n    'score': 0.0464135021,\n    'tokens': ['Johannes,', 'Dei', 'gratia,', 'Francorum', 'rex.', 'Notum', 'facimus', 'universis,', 'tam', 'presentibus', 'quam', 'futuris,', 'nobis,', 'ex', 'parte', 'Petri', 'juvenis', 'sentiferi', 'qui', 'bene', 'et', 'fideliter', 'in', 'guerris', 'nostris', 'nobis', 'servivit', 'expositum', 'fuisse,', 'qod', 'cum', 'ipse,', 'tam', 'nomine', 'suo'],\n    'tags': [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0],\n    'language': 'FR'\n},\n{\n    'key': 'FR/FR1/499.txt',\n    'start_token_id': 30,\n    'score': 0.0204918033,\n    'tokens': ['cum', 'ipse,', 'tam', 'nomine', 'suo', 'quam', 'ut', 'tutor', 'et', 'ha', 'bens', 'gubernacionem', 'seu', 'ballum', 'fratrum', 'et', 'sororum', 'suorum', 'in', 'minori', 'etate', 'constitutorum,', 'possessionem', 'aliquorum', 'bonorum', 'mobi', 'lium', 'et', 'inmobilium', 'apprehenderit,', 'quorum', 'possessionem', 'Thomas', 'juvenis', 'pater'],\n    'tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0],\n    'language': 'FR'\n}\n```\n\nEach sample specifies `key`, `start_token_id`, `score`, `tokens`, `tags`, and\n`language`. The `key` links the sample to the text file the sequence was taken\nfrom. `start_token_id` is used to merge the sequences, so we get predictions for\nall tokens in the text. This way, performance can be calculated for complete\ntexts instead of sequences. `score` (normalized editdistance) is used for\nselecting high qualitity data. For the first experiment, sequences with a\nnormalized editdistance > 0.3 were removed from the train and validation sets\n(but not from the test set!). `tokens` and `tags` contain the data that is used\nto train the classifier. `language` was not used for the first experiment.\n\n## The model\n\nThe code for training the model can be found in\n[this notebook](https://github.com/jvdzwaan/ocrpostcorrection-notebooks/blob/main/colab/icdar-task1-hf-train.ipynb).\nAfter loading the dataset, there is one more detail that needs to be taken care\nof. BERT uses subword tokenization, while the dataset contains labels for\ncomplete words. Also, BERT tokenizers add special tokens `[CLS]` and `[SEP]`.\nThis means that after BERT tokenization, the input labels don't match the tokens\nanymore, e.g.,\n\nInput sequence (and labels):\n```\n{\n    'tokens': ['This', 'is', 'a', 'cxample...']\n    'tags': [0, 0, 1, 1]\n}\n```\n\nBecause the ICDAR dataset is multilingual, I selected\n[`bert-base-multilingual-cased`](https://huggingface.co/bert-base-multilingual-cased)\nas a base model. Tokenized with the `bert-base-multilingual-cased` tokenizer the\nsequence becomes:\n```\n['[CLS]', 'This', 'is', 'a', 'c', '##xa', '##mp', '##le', '.', '.', '.', '[SEP]']\n```\n\nTo be able to train the model, the labels will have to be realigned. The\n[Hugging Face task guide on token classification](https://jvdzwaan.github.io/ocrpostcorrection/token_classification.html#tokenize_and_align_labels)\ncontains an example `tokenize_and_align` function for doing so. [A slightly\nadapted version was added to the ocrpostcorrection package.](https://huggingface.co/docs/transformers/tasks/token_classification#preprocess) This function is a\n[partial](https://docs.python.org/3/library/functools.html#functools.partial),\nallowing the tokenizer to be instantiated separately. This makes it more\nconvenient to apply it to a dataset using the\n[`Dataset.map`](https://huggingface.co/docs/datasets/process#map) function,\nbecause there is no need to add a\n[lambda function](https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions).\nTo use the function, do:\n\n```python\nfrom ocrpostcorrection.token_classification import tokenize_and_align_labels\n\ntokenized_icdar = icdar_dataset.map(tokenize_and_align_labels(tokenizer), batched=True)\n```\n\nAfter preparing the dataset, and instantiating a\n[data collator](https://huggingface.co/docs/transformers/main_classes/data_collator#transformers.DataCollatorForTokenClassification),\n[model](https://huggingface.co/bert-base-multilingual-cased) and\n[trainer](https://huggingface.co/docs/transformers/main_classes/trainer#trainer),\ntraining can start. For this experiment, the model was trained on\n[Google Colab](https://colab.research.google.com),\nusing the following training arguments:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir='choose/your/own/output/directory',\n    evaluation_strategy='epoch',\n    num_train_epochs=3,\n    load_best_model_at_end=True,\n    save_strategy='epoch',\n    per_device_train_batch_size=16\n)\n```\n:::\n\n\nThe best model came from epoch 2. For this model, training and validation loss\nwere 0.254 and 0.291, respectively.\n\n## Results\n\nThe code that was used to determine the performance of the model can be found in\n[this notebook](https://github.com/jvdzwaan/ocrpostcorrection-notebooks/blob/main/colab/icdar-task1-hf-evaluation.ipynb).\nPerformance is calculated using the competition evaluation script. This script\nexpects input in the form:\n\n```\n{\n    \"<language>/<set>/<number>.txt\":\n        {\n            \"0:1\": {},\n            \"4:2\": {},\n            ...\n        }\n    ...\n}\n```\n\nThe first number in the keys for a text represents the start index of the OCR\nmistake. The second number is the number of (input) tokens that are incorrect.\nThe evaluation script calculates precision, recall and F-measure on the token\nlevel.\n\nIt takes quite some steps to transform the (sub)token-level predictions\nthat the model provides as output into the format accepted by the evaluation\nscript. First, predictions for subtokens are merged into predictions for\n`InputToken`s. An `InputToken` is considered an OCR mistake if at least one\nsubtoken is predicted to be an OCR mistake. Next, sequences of\n`InputToken`-level predictions are merged into predictions for an entire text.\nIf predictions for overlapping `InputToken`s differ, it is considered as an OCR\nmistake. Finally, the predictions for individual tokens are translated to\n`character offset:number of tokens`-pairs. The\n[predictions2icdar_output](https://jvdzwaan.github.io/ocrpostcorrection/utils.html#predictions2icdar_output)\nfunction is available for this conversion process. It takes as input the\ntokenized test set, the predicted labels, the tokenizer, and a dictionary with\n`<file name>: Text` pairs, and returns the expected ICDAR output format:\n\n```python\nfrom ocrpostcorrection.utils import predictions2icdar_output, predictions_to_labels\n\noutput = predictions2icdar_output(tokenized_icdar['test'],\n                                  predictions_to_labels(predictions),\n                                  tokenizer,\n                                  data_test)\n```\n\nWhen saved to a JSON file, the `output` dictionary can be used to calculate\nperformance using the\n[runEvaluation](https://jvdzwaan.github.io/ocrpostcorrection/utils.html#runevaluation)\nfunction. The `runEvaluation` code was taken from the original\n`evalTool_ICDAR2017.py` (CC0 License) via\n[Kotwic4/ocr-correction](https://github.com/Kotwic4/ocr-correction/blob/master/ocr_correction/dataset/icdar/evalTool_ICDAR2017.py).\nIn addition to the JSON file, the function requires the (path to\nthe) test set as input. The function creates a csv file containing precision,\nrecall, and F-measure for all texts in the test set.\nThe following table contains the mean results grouped by language.\n\n| Language   |   Precision |   Recall |   F-measure | F-measure CCC (2019 competition winner) |\n|:-----------|---------------:|------------:|-------------:| -------------:|\n| BG         |           0.88 |        0.67 |         0.74 | **0.77**\t|\n| CZ         |           0.81 |        0.55 |         0.64 | **0.70**\t|\n| DE         |           0.98 |        0.89 |         0.93 | **0.95**\t|\n| EN         |           0.85 |        0.54 |         0.62 | **0.67**\t|\n| ES         |           0.91 |        0.46 |         0.59 | **0.69**\t|\n| FI         |           0.89 |        0.77 |         0.82 | **0.84**\t|\n| FR         |           0.81 |        0.49 |         0.59 | **0.67**\t|\n| NL         |           0.87 |        0.60 |         0.66 | **0.71**\t|\n| PL         |           0.89 |        0.70 |         0.77 | **0.82**\t|\n| SL         |           0.80 |        0.58 |         0.64 | **0.69**   |\n\nThe last column of the table reports the mean F-measure for CCC, the 2019\ncompetition winner. CCC outperforms the new model on every language, although,\nfor some languages the difference is quite small. However, for a first attempt,\nI think the results are not bad at all!\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}